{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14153716,"sourceType":"datasetVersion","datasetId":9021081}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vehicle Detection with YOLO\n\nThis notebook demonstrates vehicle detection and tracking using YOLO11. We'll process a video to detect and track vehicles (cars, buses, trucks, motorcycles, bicycles, and trains).\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Install Required Library\n\nInstall the Ultralytics library which provides the YOLO model implementation.\n","metadata":{}},{"cell_type":"code","source":"!pip install -q numpy==1.26.4 scipy==1.11.4 ultralytics opencv-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:00:47.203834Z","iopub.execute_input":"2025-12-14T16:00:47.204696Z","iopub.status.idle":"2025-12-14T16:02:22.169217Z","shell.execute_reply.started":"2025-12-14T16:00:47.204663Z","shell.execute_reply":"2025-12-14T16:02:22.168390Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Step 2: Check GPU Availability\n\nVerify if CUDA (GPU) is available for faster processing. This will help determine if we can use GPU acceleration for model inference.\n","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2025-12-14T16:02:22.170794Z","iopub.execute_input":"2025-12-14T16:02:22.171132Z","iopub.status.idle":"2025-12-14T16:02:25.158014Z","shell.execute_reply.started":"2025-12-14T16:02:22.171103Z","shell.execute_reply":"2025-12-14T16:02:25.157209Z"},"trusted":true},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Step 3: Import Required Libraries\n\nImport the necessary libraries:\n- `cv2`: OpenCV for video processing and visualization\n- `YOLO`: Ultralytics YOLO model for object detection and tracking\n- `defaultdict`: For tracking vehicle counts (if needed)\n","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nfrom collections import defaultdict","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-12-14T16:02:25.158947Z","iopub.execute_input":"2025-12-14T16:02:25.159348Z","iopub.status.idle":"2025-12-14T16:02:28.373643Z","shell.execute_reply.started":"2025-12-14T16:02:25.159326Z","shell.execute_reply":"2025-12-14T16:02:28.372962Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 4: Load YOLO Model\n\nLoad the pre-trained YOLO11 Large model. This model is capable of detecting and tracking multiple object classes including vehicles.\n","metadata":{}},{"cell_type":"code","source":"model = YOLO('yolo11l.pt')","metadata":{"execution":{"iopub.status.busy":"2025-12-14T16:02:28.374414Z","iopub.execute_input":"2025-12-14T16:02:28.374684Z","iopub.status.idle":"2025-12-14T16:02:29.274836Z","shell.execute_reply.started":"2025-12-14T16:02:28.374667Z","shell.execute_reply":"2025-12-14T16:02:29.273884Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt to 'yolo11l.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 49.0MB 139.2MB/s 0.4s0.3s<0.0s\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Step 5: View Available Classes\n\nDisplay the list of all classes that the YOLO model can detect. This helps us understand which class IDs correspond to vehicles (e.g., car=2, bus=5, truck=7).\n","metadata":{}},{"cell_type":"code","source":"class_list = model.names\nclass_list","metadata":{"execution":{"iopub.status.busy":"2025-12-14T16:02:29.276587Z","iopub.execute_input":"2025-12-14T16:02:29.276805Z","iopub.status.idle":"2025-12-14T16:02:29.283683Z","shell.execute_reply.started":"2025-12-14T16:02:29.276788Z","shell.execute_reply":"2025-12-14T16:02:29.282833Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{0: 'person',\n 1: 'bicycle',\n 2: 'car',\n 3: 'motorcycle',\n 4: 'airplane',\n 5: 'bus',\n 6: 'train',\n 7: 'truck',\n 8: 'boat',\n 9: 'traffic light',\n 10: 'fire hydrant',\n 11: 'stop sign',\n 12: 'parking meter',\n 13: 'bench',\n 14: 'bird',\n 15: 'cat',\n 16: 'dog',\n 17: 'horse',\n 18: 'sheep',\n 19: 'cow',\n 20: 'elephant',\n 21: 'bear',\n 22: 'zebra',\n 23: 'giraffe',\n 24: 'backpack',\n 25: 'umbrella',\n 26: 'handbag',\n 27: 'tie',\n 28: 'suitcase',\n 29: 'frisbee',\n 30: 'skis',\n 31: 'snowboard',\n 32: 'sports ball',\n 33: 'kite',\n 34: 'baseball bat',\n 35: 'baseball glove',\n 36: 'skateboard',\n 37: 'surfboard',\n 38: 'tennis racket',\n 39: 'bottle',\n 40: 'wine glass',\n 41: 'cup',\n 42: 'fork',\n 43: 'knife',\n 44: 'spoon',\n 45: 'bowl',\n 46: 'banana',\n 47: 'apple',\n 48: 'sandwich',\n 49: 'orange',\n 50: 'broccoli',\n 51: 'carrot',\n 52: 'hot dog',\n 53: 'pizza',\n 54: 'donut',\n 55: 'cake',\n 56: 'chair',\n 57: 'couch',\n 58: 'potted plant',\n 59: 'bed',\n 60: 'dining table',\n 61: 'toilet',\n 62: 'tv',\n 63: 'laptop',\n 64: 'mouse',\n 65: 'remote',\n 66: 'keyboard',\n 67: 'cell phone',\n 68: 'microwave',\n 69: 'oven',\n 70: 'toaster',\n 71: 'sink',\n 72: 'refrigerator',\n 73: 'book',\n 74: 'clock',\n 75: 'vase',\n 76: 'scissors',\n 77: 'teddy bear',\n 78: 'hair drier',\n 79: 'toothbrush'}"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Step 6: Open Video File\n\nOpen the input video file for processing. Make sure to update the path to match your video file location.\n","metadata":{}},{"cell_type":"code","source":"cap = cv2.VideoCapture('/kaggle/input/test-video-for-vehicle-counting/Test Video for Vehicle Counting Model.mp4')","metadata":{"execution":{"iopub.status.busy":"2025-12-14T16:02:29.284541Z","iopub.execute_input":"2025-12-14T16:02:29.284848Z","iopub.status.idle":"2025-12-14T16:02:29.313614Z","shell.execute_reply.started":"2025-12-14T16:02:29.284813Z","shell.execute_reply":"2025-12-14T16:02:29.312839Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Step 7: Video Processing Setup\n\nConfigure logging and visualization parameters for the video processing pipeline.\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport logging\n\n# ğŸ”‡ Silence Ultralytics logs\nlogging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n\n# ---------------- CONFIG ----------------\nfont_scale_label = 0.45\nbox_thickness = 2\n# ----------------------------------------\n\nclass_list = model.names\n\n# Video writer\nsave_output = True\nif save_output:\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(\n        \"output.mp4\",\n        fourcc,\n        30,\n        (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n         int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    )\n\n# ---------------- MAIN LOOP ----------------\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    results = model.track(\n        frame,\n        persist=True,\n        classes=[1, 2, 3, 5, 6, 7],\n        verbose=False\n    )\n\n    if results[0].boxes is not None and results[0].boxes.id is not None:\n        boxes = results[0].boxes.xyxy.cpu()\n        track_ids = results[0].boxes.id.int().cpu().tolist()\n        class_indices = results[0].boxes.cls.int().cpu().tolist()\n\n        for box, track_id, class_idx in zip(boxes, track_ids, class_indices):\n            x1, y1, x2, y2 = map(int, box)\n            cx = (x1 + x2) // 2\n            cy = (y1 + y2) // 2\n\n            class_name = class_list[class_idx]\n\n            # Bounding box\n            cv2.rectangle(\n                frame,\n                (x1, y1),\n                (x2, y2),\n                (0, 200, 0),\n                box_thickness\n            )\n\n            # Center point\n            cv2.circle(frame, (cx, cy), 3, (0, 0, 255), -1)\n\n            # Label\n            cv2.putText(\n                frame,\n                f\"{class_name} | ID {track_id}\",\n                (x1, max(y1 - 6, 15)),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                font_scale_label,\n                (255, 255, 255),\n                1\n            )\n\n    if save_output:\n        out.write(frame)\n\n# ---------------- CLEANUP ----------------\ncap.release()\nif save_output:\n    out.release()\n\nprint(\"âœ… Video processed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:02:29.314512Z","iopub.execute_input":"2025-12-14T16:02:29.315130Z","iopub.status.idle":"2025-12-14T16:05:00.129086Z","shell.execute_reply.started":"2025-12-14T16:02:29.315112Z","shell.execute_reply":"2025-12-14T16:05:00.128284Z"}},"outputs":[{"name":"stdout","text":"âœ… Video processed successfully.\n","output_type":"stream"}],"execution_count":7}]}